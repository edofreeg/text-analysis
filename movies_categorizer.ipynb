{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oluiHL4a9_EG"
   },
   "source": [
    "# **MOVIE PLOTS CLASSIFICATION PROJECT**\n",
    "Students:\n",
    "*Edoardo Frigerio, Mattia De Masi, Lorenzo Scandolara*\n",
    "Professor: Andrea Belli\n",
    "Subject: Text Mining\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "This project is based on a dataset freely available on github at https://media.githubusercontent.com/media/nluninja/nlp_datasets/main/wikipedia_movie_plots/data/wiki_movie_plots_deduped.csv .\n",
    "\n",
    "The dataset is composed by 34886 observations gathered from 1901 to 2017. For each observation we have some relevant features such as the title, year release, genre and the plot from wikipedia.\n",
    "\n",
    "Our aim is to train a model in order to be able to classify plots of movies with different labels corresponding to different movies genre. We have used 2 different architecture (***LSTM*** and ***BERT***) and we have tried to compare these models to understand strengths and weaknesses for each.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iDi5xycNL84"
   },
   "source": [
    "## (1) Import libraries\n",
    "This cell is used to import all the necessary packages and libraries that we have used in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NcINulu9l_Oe"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install --upgrade tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRlCVBYsMqVF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import spacy \n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, TimeDistributed, Dropout, Bidirectional, InputLayer, concatenate, Reshape\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import torch\n",
    "import re\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "import string\n",
    "import itertools\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AU87wf00Rjhi"
   },
   "source": [
    "## (2) Load the Data\n",
    "Our dataset is made up of 34886 rows and 8 columns. The columns represent:\n",
    "* Release Year\n",
    "* Title\n",
    "* Origin/Ethnicity\n",
    "* Director\n",
    "* Cast\n",
    "* Genre\n",
    "* Wiki Page\n",
    "* Plot\n",
    "\n",
    "Our aim is find the movie's Genre starting from the description in the Plot column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2jBGsYxNAbg"
   },
   "outputs": [],
   "source": [
    "movies = pd.read_csv('https://media.githubusercontent.com/media/nluninja/nlp_datasets/main/wikipedia_movie_plots/data/wiki_movie_plots_deduped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "id": "9huz-NHCOpLu",
    "outputId": "d8af8554-5393-4b76-95e6-6c36260a575d"
   },
   "outputs": [],
   "source": [
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZYilv_QgC3oF",
    "outputId": "04e1b826-712c-4df5-d23b-a63ece8973ca"
   },
   "outputs": [],
   "source": [
    "len(movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDofmqto-4zA"
   },
   "source": [
    "## (3) Data cleaning\n",
    "Let's start to clean the dataset. \n",
    "We are going to remove all columns which are not useful for our task. We end up with a dataset composed only with \"Genre\", \"Release Year\" and \"Plot\" colums form the previous dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bxh4N8--NAeg"
   },
   "outputs": [],
   "source": [
    "movies_edit = movies[[\"Genre\",\"Release Year\",\"Plot\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjTQrb65-E-o"
   },
   "source": [
    "Let's remove nuisances as unknown values or duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WW8auRBxKAfT"
   },
   "outputs": [],
   "source": [
    "i = movies_edit[(movies_edit.Genre == 'unknown')].index\n",
    "movies_edit=movies_edit.drop(i)\n",
    "i = movies_edit[(movies_edit.Genre ==' ') ].index\n",
    "movies_edit=movies_edit.drop(i)\n",
    "movies_edit=movies_edit.drop_duplicates(subset=['Plot'])\n",
    "movies_edit=movies_edit.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "vXPJqkGKDTl_",
    "outputId": "3224ea31-4e97-4775-b445-69a69bd75bee"
   },
   "outputs": [],
   "source": [
    "movies_edit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4BMsI-AVqAt"
   },
   "source": [
    "We dropped 6561 observations from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R_ViTO4hhWky",
    "outputId": "34494c97-f1af-4028-b42b-6d7e3f6c4830"
   },
   "outputs": [],
   "source": [
    "len(movies) - len(movies_edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x-ZaBEVYFx-i"
   },
   "source": [
    "### (3.1) Aggregation of classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fI2vrkrWRyOJ"
   },
   "source": [
    "'Genre' is a general classification with possibly more than one label for each observation, we want to reduce the 'Genre' column so that we have only one single label for each observation. To do so we will add a new column to the dataset called 'First_genre', as we are assuming that the first reported genre is the most relevant for each movie.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nnm9xRAul_4U"
   },
   "outputs": [],
   "source": [
    "First_Genre=[]\n",
    "for i in range(len(movies_edit)):\n",
    "    split_genre=movies_edit.loc[i,'Genre'].split(\",\")\n",
    "    First_Genre.append(split_genre[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0vwU6XKrQtX"
   },
   "outputs": [],
   "source": [
    "for i in range(len(First_Genre)):\n",
    "    split_genre=First_Genre[i].split(\"/\")\n",
    "    First_Genre[i] = split_genre[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6ZuVeL0_f2H"
   },
   "source": [
    "Our aim is to create 5 macro-classes with the biggest possible number of observation each. In the dataset we have some labels for the 'Genre' column that we want to aggregate with the most popular classes that we will lately use in the models training. The cell below shows the aggregation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QxMppZFdtZMh"
   },
   "outputs": [],
   "source": [
    "for i in range(len(First_Genre)):\n",
    "  if First_Genre[i] == \"action \":\n",
    "     First_Genre[i] = (\"action\")\n",
    "  elif First_Genre[i] == \"world war ii\" or First_Genre[i] == \"war\":\n",
    "    First_Genre[i] = (\"action\")\n",
    "  elif First_Genre[i] == \"crime\":\n",
    "    First_Genre[i] = (\"thriller\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSk4nc8gAfBX"
   },
   "source": [
    "Then we finally add this new column to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gr99bNrYFfsR"
   },
   "outputs": [],
   "source": [
    "movies_edit[\"First_genre\"] = First_Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7UZM28F-NAmf",
    "outputId": "67cbcfce-6e40-4738-f09a-55e1cd3bdae1"
   },
   "outputs": [],
   "source": [
    "movies_edit['First_genre'].value_counts().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZGlG0t-GYwc"
   },
   "outputs": [],
   "source": [
    "labels_perc = movies_edit['First_genre'].value_counts()/len(movies_edit['First_genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mAkO-d_D3-xI",
    "outputId": "63ed22ea-7c03-4571-ca9e-ac824b60f173"
   },
   "outputs": [],
   "source": [
    "labels_perc.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flNt_u-iJRqm"
   },
   "source": [
    "We have almost **60%** of observation within the first 5 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XOm3zmVeJGzE",
    "outputId": "cdb889e2-9252-47ce-f51a-a947d17d3227"
   },
   "outputs": [],
   "source": [
    "labels_perc[0:5].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hl2ijAYsGsTZ"
   },
   "source": [
    "Displaying frequency for each **relevant** class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 325
    },
    "id": "iYkRpYEVNApJ",
    "outputId": "4e72ae91-9e74-45af-cf10-936aca6af400"
   },
   "outputs": [],
   "source": [
    "labels_perc[:5].plot.bar()\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Relative Frequency')\n",
    "plt.title('Bar chart for classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tK0tJWrxAs-N"
   },
   "source": [
    "Now we create a new dataset composed only by the 5 classes mentioned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0C2fiEjV6Z6q"
   },
   "outputs": [],
   "source": [
    "movies_edit2 = movies_edit.groupby(\"First_genre\").filter( lambda x: len(x)>=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zN22GB5M6lMi",
    "outputId": "be9c16a9-e1d5-40e1-83c6-b18a520cf4a5"
   },
   "outputs": [],
   "source": [
    "movies_edit2['First_genre'].value_counts()/len(movies_edit2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALiu_rHmGvHv"
   },
   "source": [
    "It is clear that this dataset is **not balanced**. We will asses this balancing problem later in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIXFgBKEJ1BX"
   },
   "source": [
    "## (4) Lemmatization \n",
    "We define a function to lemmatize words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXp-IbXtWied"
   },
   "outputs": [],
   "source": [
    "movies_edit2['clean_plot']=movies_edit2['Plot'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VgkDmfJAVeiW"
   },
   "outputs": [],
   "source": [
    "def getLemmText(text):\n",
    " tokens=word_tokenize(text)\n",
    " lemmatizer = WordNetLemmatizer()\n",
    " tokens=[lemmatizer.lemmatize(word) for word in tokens]\n",
    " return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lpMgyVr2WgYa"
   },
   "outputs": [],
   "source": [
    "movies_edit2['clean_plot'] = list(map(getLemmText,movies_edit2['clean_plot']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfR7fbA0pHn1"
   },
   "source": [
    "**Stop words**\n",
    "\n",
    "The package nltk provide us a list of the most common words in english. We decide to add in this list some words that we've noticed are recurrent in all the plot and so are not useful for classification: \n",
    "* film\n",
    "* movie\n",
    "* go\n",
    "* one\n",
    "* two\n",
    "* take\n",
    "* get\n",
    "* find\n",
    "* back\n",
    "* tell\n",
    "* ha\n",
    "* wa\n",
    "* and other unuseful words\n",
    "\n",
    "Our aim is remove all the stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzhAeaCiMeAF"
   },
   "outputs": [],
   "source": [
    "movies_edit2 = movies_edit2.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;.]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = stopwords.words('english')\n",
    "STOPWORDS.append('film')\n",
    "STOPWORDS.append('movie')\n",
    "STOPWORDS.append('one')\n",
    "STOPWORDS.append('two')\n",
    "STOPWORDS.append('get')\n",
    "STOPWORDS.append('back')\n",
    "STOPWORDS.append('tell')\n",
    "STOPWORDS.append('new')\n",
    "STOPWORDS.append('also')\n",
    "STOPWORDS.append('time')\n",
    "STOPWORDS.append('hi')\n",
    "STOPWORDS.append('ha')\n",
    "STOPWORDS.append('wa')\n",
    "STOPWORD = set(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6vihmnoi8NF8"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvedw6XUONoe"
   },
   "outputs": [],
   "source": [
    "movies_edit2['clean_plot'] = movies_edit2['clean_plot'].apply(clean_text)\n",
    "movies_edit2['clean_plot'] = movies_edit2['clean_plot'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vs8pG00GK_Lg"
   },
   "source": [
    "We want to understand what are the most common stop words, we use a bar chart in order to visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3j-vDohi8sN"
   },
   "outputs": [],
   "source": [
    "#Counting and displaying the occurrences of a list of stopwords\n",
    "nlplist = []\n",
    "for i in range(len(movies_edit2['Plot'])):\n",
    "    nlplist.append(str(movies_edit2['Plot'][i]))\n",
    "listToStr = ' '.join([str(elem) for elem in nlplist])\n",
    "\n",
    "Dict_stop = {}\n",
    "for word in STOPWORD:\n",
    "     Dict_stop[word] = listToStr.count(word)\n",
    "\n",
    "top30 =sorted(Dict_stop.items(), key=lambda x:x[1],reverse=True)[:30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "Guq48Qcl8Qw0",
    "outputId": "94e20e44-2cc1-4b3d-8e78-7693010faf98"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "x,y=zip(*top30) # x is the tuple containing the 30 most common words, y is a tuple containing counts\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Words')\n",
    "plt.title('30 most common Stopwords')\n",
    "plt.bar(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvaCQi_xDrNb"
   },
   "source": [
    "Counting the stopwords for each Genre\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zz3cIyX3NA2w"
   },
   "outputs": [],
   "source": [
    "def count_stopwords(x):\n",
    "    n_stopwords = 0\n",
    "\n",
    "    for word in str(x).split(' '):\n",
    "        if word in STOPWORDS:\n",
    "            n_stopwords += 1\n",
    "\n",
    "    return n_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2MmYMFhIJJMQ"
   },
   "source": [
    "We show how many stopwords in average per Genre are inside the 'clean_plot'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tin5_ysnNA5Z"
   },
   "outputs": [],
   "source": [
    "movies_edit2[\"stopwords\"] = movies_edit2[\"Plot\"].apply(count_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "id": "AzY7P9ekNA-g",
    "outputId": "6b026039-140a-45ab-eda0-132ac64663f8"
   },
   "outputs": [],
   "source": [
    "stopwords_label = movies_edit2.groupby(by='First_genre')['stopwords'].agg('mean')\n",
    "stopwords_label.plot.bar(title = \"mean stopwords per genre\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G53-8lLuEWBx"
   },
   "source": [
    "Now we define the function to remove all the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "nSwEdgwzNBD3",
    "outputId": "df053dc8-ee70-4c6d-9e91-72c53e9ed068"
   },
   "outputs": [],
   "source": [
    "movies_edit2.drop('index', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2m_R62Qs1zm"
   },
   "source": [
    "In the two line below we can see the differences between the starting plot and the final clean_plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2012PCa6iP4s",
    "outputId": "51fff8a3-92cf-43a5-b97c-ac53ec9c9bbd"
   },
   "outputs": [],
   "source": [
    "print(movies_edit2.loc[1][\"Plot\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddGxLPTgjx1X",
    "outputId": "4373ebbc-af6d-4430-8cc4-8b0e7039dff4"
   },
   "outputs": [],
   "source": [
    "print(movies_edit2.loc[1][\"clean_plot\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lLc24Dm9fir"
   },
   "outputs": [],
   "source": [
    "list_plot = movies_edit2[\"clean_plot\"].to_list()\n",
    "len_plot = [len(str(plot).split(\" \")) for plot in list_plot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jiRmlmrm9fbj"
   },
   "outputs": [],
   "source": [
    "movies_edit2['clean_plot'] = movies_edit2['clean_plot'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWLwOSPfICzo"
   },
   "source": [
    "As we said before we have to balance the dataset. We have decided to undersample the dataset. Doing so we have randomly taken 1260 observations from each macro-class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "coY6u5IVVpXb"
   },
   "outputs": [],
   "source": [
    "movies_edit3=pd.concat([movies_edit2.loc[movies_edit2[\"First_genre\"]==\"comedy\"].sample(n=1260,replace=False,random_state=1200),\n",
    "                        movies_edit2.loc[movies_edit2[\"First_genre\"]==\"drama\"].sample(n=1260,replace=False,random_state=1200),\n",
    "                        movies_edit2.loc[movies_edit2[\"First_genre\"]==\"horror\"],\n",
    "                        movies_edit2.loc[movies_edit2[\"First_genre\"]==\"thriller\"].sample(n=1260,replace=False,random_state=1200),\n",
    "                        movies_edit2.loc[movies_edit2[\"First_genre\"]==\"action\"].sample(n=1260,replace=False,random_state=1200)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HW4I190ILeNn",
    "outputId": "02e6dd89-b60d-4d1d-ebf8-8fba9fbec958"
   },
   "outputs": [],
   "source": [
    "movies_edit3['First_genre'].value_counts()/len(movies_edit3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SC-uj5KBK1G6"
   },
   "source": [
    "## (5) Graphical overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93gh9KXkLLkj"
   },
   "source": [
    "#### (5.1) **Number of movies per 5 years span**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "iqFpSaRK5WXh",
    "outputId": "54f6e27f-3734-40e3-de2b-23cfae39c193"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5));\n",
    "\n",
    "movies_edit3[\"Release Year\"].plot.hist(stacked=True, bins=20)\n",
    "plt.xlabel('YEARS')\n",
    "plt.ylabel('# MOVIES')\n",
    "plt.title('NUMBER OF MOVIES PER 5 YEARS SPAN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mnnf8XuWaIPr"
   },
   "source": [
    "#### (5.2) **Number of words in each plot**\n",
    "The graph is in range (0,2000) to make it more readable but there are some plots with more than 4000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiEOzSqnbE1l"
   },
   "outputs": [],
   "source": [
    "list_plot = movies_edit3[\"Plot\"].to_list()\n",
    "len_plot = [len(str(plot).split(\" \")) for plot in list_plot]\n",
    "movies_edit3[\"len_plot\"] = len_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "RPppt7fLaSWI",
    "outputId": "c3da19bb-442f-497d-a928-81c959ef406a"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5));\n",
    "movies_edit3[\"len_plot\"].plot.hist(stacked=True, bins=20, range=(0,2000))\n",
    "plt.xlabel('NUMBER OF WORDS')\n",
    "plt.ylabel('NUMBER OF PLOT')\n",
    "plt.title('NUMBER OF WORDS IN EACH PLOT')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAghTTvwLTh8"
   },
   "source": [
    "#### (5.3) **Number of words per category** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "id": "lTXmyj1jLEM4",
    "outputId": "586241cb-fc37-43b5-9a72-ec5d54a620d9"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5));\n",
    "len_plot_label = movies_edit3.groupby(by='First_genre')['len_plot'].agg('mean')\n",
    "len_plot_label.plot.bar(title = \"WORDS PER LABEL (MEAN)\", ylabel = '# OF WORDS', xlabel = 'GENRE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_VrlX0FL5Ka"
   },
   "source": [
    "#### (5.4) **Number of stopwords per Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "id": "4VAvVAMAL5eW",
    "outputId": "2960cf9b-b2e6-4b18-feb6-2d4270950570"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5));\n",
    "stopwords_label = movies_edit3.groupby(by='First_genre')['stopwords'].agg('mean')\n",
    "stopwords_label.plot.bar(title = 'STOPWORDS PER GENRE')\n",
    "plt.xlabel('FIRST GENRE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMYM3NRSMHMM"
   },
   "source": [
    "#### (5.5) **Percentage of stopwords per Genre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "id": "AFw3aVsIMHjN",
    "outputId": "26bae457-f3d9-4b0f-8aa4-96f4fbd6dab8"
   },
   "outputs": [],
   "source": [
    "movies_edit3[\"perc_stopwords\"] = movies_edit3[\"stopwords\"]/movies_edit3[\"len_plot\"]\n",
    "\n",
    "plt.figure(figsize=(10,5));\n",
    "stopwords_perc = movies_edit3.groupby(by='First_genre')['perc_stopwords'].agg('mean')\n",
    "stopwords_perc.plot.bar(title = \"% OF STOPWORDS PER LABEL\")\n",
    "plt.xlabel('FIRST GENRE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mz5V_5KQNZi1"
   },
   "source": [
    "#### (5.6) **Most used word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzmBGTB1p_qp"
   },
   "outputs": [],
   "source": [
    "movies_edit3 = movies_edit3.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "id": "5PjptX1ppjDX",
    "outputId": "34f911cf-61fe-47ff-df08-87b8536ee93a"
   },
   "outputs": [],
   "source": [
    "#which words are the most frequent in the dataset without stopwords\n",
    "movies_stop = movies_edit3[['First_genre','clean_plot']].copy() #create a copy of df and removing stopwords\n",
    "STOPWORDS = set(nlp.Defaults.stop_words)\n",
    "\n",
    "movies_stop['PLOT'] = movies_stop['clean_plot'].apply(lambda x:str(x).split())\n",
    "\n",
    "freq_words = Counter([word for text in movies_stop['PLOT'] for word in text if len(word)>2])\n",
    "freq_word_df = pd.DataFrame(freq_words.most_common(150), columns=['Word','Count'])\n",
    "plt.figure(figsize=(20,10));\n",
    "sns.barplot(y='Word',x='Count',data=freq_word_df[:25], color= 'tab:blue').set_title(\"25 MOST FREQUENT WORDS IN TEXT\")\n",
    "plt.xlabel('COUNT')\n",
    "plt.ylabel('WORD')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DeTHPCs5OajV"
   },
   "outputs": [],
   "source": [
    "most_frequent = movies_edit3.groupby(\"First_genre\")[\"clean_plot\"].apply(lambda x: Counter(\" \".join(x).split()).most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RAixn5TtiDvy",
    "outputId": "3577599f-fdff-4530-d96b-73e56cfbb129"
   },
   "outputs": [],
   "source": [
    "genre = sorted(movies_edit3['First_genre'].unique())\n",
    "for i in range(len(genre)):\n",
    "  q=[]\n",
    "  print(genre[i],\":\")\n",
    "  for j in range(20):\n",
    "    q.append(most_frequent[i][j][0])\n",
    "  print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4KDSKS57pDu"
   },
   "source": [
    "## (6) Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVCd8nP4cQIx"
   },
   "source": [
    "### (6.1) LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6fB8jgXlv_S"
   },
   "source": [
    "Defining parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SQSENcQmbLcp",
    "outputId": "8fc31923-ef01-4517-b093-4a3bde782d64"
   },
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 6000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "EMBEDDING_DIM = 100\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(movies_edit3['clean_plot'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tkdWIttpbO_R",
    "outputId": "d0cd8447-6c1f-415f-b384-a9a7f82e6ee8"
   },
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(movies_edit3['clean_plot'].values)\n",
    "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0xLhwgGWbPB4",
    "outputId": "3f3f8235-4a2f-4467-b396-0ea6713b4c8c"
   },
   "outputs": [],
   "source": [
    "Y = pd.get_dummies(movies_edit3['First_genre']).values\n",
    "print('Shape of label tensor:', Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6_oTAYGbPE0",
    "outputId": "292b52f5-98c1-4423-8601-944e8d20ba3d"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HziPUsTNlzvR"
   },
   "source": [
    "Defining model architecture:\n",
    "\n",
    "- Embedding layer\n",
    "\n",
    "- Dropout layer ( SpatialDropout1D will help promote independence between feature map)\n",
    "\n",
    "- LSTM layer\n",
    "\n",
    "- Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5-ObRwvbPHs"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0K_VXecl3rc"
   },
   "outputs": [],
   "source": [
    "epochs = 6\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lMlL3H7tl9Tt"
   },
   "source": [
    "#### (6.1.1) Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hJHgA4z9bPLL",
    "outputId": "d9efa79a-026b-4a9a-93d0-9242d18991e3"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FzXxFNY-bVF4",
    "outputId": "7492559f-1f55-41a9-f0ce-9ec7114c2d7b"
   },
   "outputs": [],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "DJ5WAb1CbVIg",
    "outputId": "d534af2c-272b-4062-f0c2-74d902fe1329"
   },
   "outputs": [],
   "source": [
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "0n0mq45Sd7gD",
    "outputId": "97381d05-ffb0-41f8-ca0f-fce519ab6311"
   },
   "outputs": [],
   "source": [
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYtd_J5HbVLf"
   },
   "outputs": [],
   "source": [
    "y_predict = np.argmax(model.predict(X_test), axis=-1)\n",
    "y_true = Y_test.argmax(axis=1)\n",
    "label_names = movies_edit3['First_genre'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yyb6h41FbVON",
    "outputId": "bdb6f38b-df5a-4c22-fd90-91cd6f3392cd"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_predict, target_names=label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6Ue4029mVDy"
   },
   "source": [
    "#### (6.1.2) Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "bsa9PnEXbckR",
    "outputId": "f3464644-ec77-49a4-d14f-608e1c8be907"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true,y_predict)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = label_names)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubvpqXpWcCd5"
   },
   "source": [
    "### (6.2) Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnPSXK79cUqc"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OoBw9fP3cpwC"
   },
   "outputs": [],
   "source": [
    "epochs = 6\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bOUDuW4mH3s"
   },
   "source": [
    "#### (6.2.1) Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xvTHukN6cUtN",
    "outputId": "cc2bd0f4-e72e-447e-ba01-da7cdc482c24"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2rlA2rcPcgMW",
    "outputId": "18173866-2de9-4436-a417-6fd1dcbc5747"
   },
   "outputs": [],
   "source": [
    "accr = model.evaluate(X_test,Y_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "KhxnHxC3cgO0",
    "outputId": "b7f3e775-3d27-4ab8-e32d-e6edc2cb32e0"
   },
   "outputs": [],
   "source": [
    "plt.title('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='test accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "85-RVro3ezeC",
    "outputId": "0804bd32-ad55-4467-f4f9-ec753107436d"
   },
   "outputs": [],
   "source": [
    "plt.title('Loss')\n",
    "plt.plot(history.history['loss'], label='train loss')\n",
    "plt.plot(history.history['val_loss'], label='test loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRLrK3trcitN"
   },
   "outputs": [],
   "source": [
    "y_predict = np.argmax(model.predict(X_test), axis=-1)\n",
    "y_true = Y_test.argmax(axis=1)\n",
    "label_names = movies_edit3['First_genre'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Q2aT79uclXX",
    "outputId": "be4ac6c8-e7ae-4456-98b3-c0b63b9a75b1"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_predict, target_names=label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqmpusRamOuA"
   },
   "source": [
    "#### (6.2.2)Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "7sx5J1toclZ6",
    "outputId": "801b59fa-c63f-4c2a-ca65-5a4554764dda"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true,y_predict)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = label_names)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ew8ToqmjQs6"
   },
   "source": [
    "## (7) Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNgPXmmPjxMC"
   },
   "source": [
    "### (7.1) Preprocessing data\n",
    "We create a new dataframe with only two columns: \n",
    "* clean_plot (our independent variable)\n",
    "* First_Genre (our response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2kFZXV5_jtDz"
   },
   "outputs": [],
   "source": [
    "df=movies_edit3[['clean_plot','First_genre']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-K1Qx4cdkRAY"
   },
   "outputs": [],
   "source": [
    "df =df.rename(columns={'clean_plot':'TITLE','First_genre':'CATEGORY'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-85aSa1q2f27"
   },
   "source": [
    "We define a new function to factorize each genre from 0 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAY_ACV0kUpC"
   },
   "outputs": [],
   "source": [
    "encode_dict = {}\n",
    "\n",
    "def encode_cat(x):\n",
    "    if x not in encode_dict.keys():\n",
    "        encode_dict[x]=len(encode_dict)\n",
    "    return encode_dict[x]\n",
    "\n",
    "df['ENCODE_CAT'] = df['CATEGORY'].apply(lambda x: encode_cat(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2psGAhx9lOx-"
   },
   "source": [
    "Defining some key variables that will be used later on in the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4y8JSVskfSl"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 100\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2 \n",
    "EPOCHS = 6\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X30uZc80kh2M"
   },
   "outputs": [],
   "source": [
    "class Triage(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.data.TITLE[index])\n",
    "        title = \" \".join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.ENCODE_CAT[index], dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5k_lLWylT_a"
   },
   "source": [
    "Creating the dataset and dataloader for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XPygwa6oklcO",
    "outputId": "e392d811-ac88-419a-b982-59afcc230d0b"
   },
   "outputs": [],
   "source": [
    "train_size = 0.6\n",
    "train_dataset=df.sample(frac=train_size,random_state=200)\n",
    "test_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = Triage(test_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1UROqdRkmGV"
   },
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KI-FubJUdIfO"
   },
   "source": [
    "### (7.2) Bert Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C6zaaglHlYYV"
   },
   "source": [
    "Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t6zYYrWKkoYN"
   },
   "outputs": [],
   "source": [
    "class DistillBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistillBERTClass, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 5)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "13f64dcd9efe42718ac576732885d7e3",
      "4fd4c4653cb44688b10aec09c3b1a1b6",
      "793b07c028b74343a2867f6e6c6a7de5",
      "c8aafd96360c4d94a2088434ee5c569c",
      "b3b0d09574ef4d04bc63a30852f8d0e8",
      "c12567896fd34f938bb72051c756ffb5",
      "a63803ebbb354613b4f3a78f07ece0ea",
      "12f0305170bd4b0b8876b2d85f8d5bf1",
      "d570d0cbe0e44ab9b52c193b09c1d0c8",
      "ab05fbdddc6f482fb0afac56488004dd",
      "02a441ee39ea42deb085177e3f972e78",
      "caaa894c966f4411b4de7a3ba5925af2",
      "dfd7eae067f049ef944cfbb87d079b92",
      "381ed03c2fb643eba28bc6329e64ca98",
      "23218c52312c4c8cb33f71b4acf8f323",
      "7fa16118807c4b25a8961f867fe2a0a5",
      "85ddba72a3c648d9a94833129ecc928a",
      "9451e7cd103a45b58af3aa337fb6eb18",
      "2dcedfc369bf41d1a44aac7a7ae13884",
      "dfc42c330b0041cca5ee0ab80430bee0",
      "c054170517f7476fa6876af5fa553077",
      "7cc02bd042a3423b87c72f0718e673e7"
     ]
    },
    "id": "yYG0HmzTkuyi",
    "outputId": "47557918-76e5-4d9f-ab57-3f91f74933ff"
   },
   "outputs": [],
   "source": [
    "model = DistillBERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIns-lLglcGj"
   },
   "source": [
    "Creating the loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NjE-hxykwxU"
   },
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVbySj2vlfQy"
   },
   "source": [
    "Function to calcuate the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LbPYOGvyk1QM"
   },
   "outputs": [],
   "source": [
    "def calcuate_accu(big_idx, targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    return n_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJ7Ox1woljxV"
   },
   "source": [
    "Defining the training function on the 80% of the dataset for tuning the distilbert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlvZowrKk09D"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask)\n",
    "        loss = loss_function(outputs, targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "        n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps += 1\n",
    "        nb_tr_examples+=targets.size(0)\n",
    "        \n",
    "        if _%100==0:\n",
    "          loss_step = tr_loss/nb_tr_steps\n",
    "          accu_step = (n_correct*100)/nb_tr_examples \n",
    "          print(f\"Training Accuracy per step {nb_tr_steps}: {accu_step}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n26xAKgkdnJK"
   },
   "source": [
    "#### (7.2.1) Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tI5A12SQk53n",
    "outputId": "d59a329c-51d0-4755-dafc-61bc9b7e3987"
   },
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yskQSu46dYfc"
   },
   "source": [
    "#### (7.2.2) Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08kRWcDfk97L"
   },
   "outputs": [],
   "source": [
    "def valid(model, testing_loader):\n",
    "    model.eval()\n",
    "    n_correct = 0; n_wrong = 0; total = 0\n",
    "    nb_tr_steps=0 ;nb_tr_examples=0 ; tr_loss=0\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask).squeeze()\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            n_correct += calcuate_accu(big_idx, targets)\n",
    "\n",
    "            nb_tr_steps += 1\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    return epoch_accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9ucQK3Q9OQU"
   },
   "source": [
    "#### (7.2.3) Accuracy report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UWKC9f3ZlD_c",
    "outputId": "603a48d0-1989-4834-c238-7cbacd696ae6"
   },
   "outputs": [],
   "source": [
    "print('This is the validation section to print the accuracy and see how it performs')\n",
    "print('Here we are leveraging on the dataloader crearted for the validation dataset, the approcah is using more of pytorch')\n",
    "\n",
    "acc = valid(model, testing_loader)\n",
    "print(\"Accuracy on test data = %0.2f%%\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSyu7qEKiZ-z"
   },
   "source": [
    "## (8) Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gj2w2K0dilyh"
   },
   "source": [
    "We work on three different architectures: \n",
    "1. LSTM\n",
    "2. Bi-LSTM\n",
    "3. BERT \n",
    "\n",
    "The first two architecture gave us very similar output but Bi-LSTM performed better, indeed we reach an accuracy of 52%, 4% better than the LSTM. \n",
    "\n",
    "Even after dropping the most recurrent words each plot has similar text. Moreover there isn't a common link between the same genre\n",
    "\n",
    "As comparison we have used accuracy for all 3 models.\n",
    "\n",
    "BERT gave us a slightly worse result: only 42,54% of test accuracy. The problem could be discernible from the fact that we have used only 6 epochs to have a comparison between models and BERT's response is less functional than the response of other two models. \n",
    "\n",
    "\n",
    "All the models that we reported here don't have an accuracy as high as in other examples of text classification. This is due to the fact that the original classification in the dataset is not as specific as it should be. A lot of movies have more than 1 genre and the classification is very broad in terms of different genre taken into consideration. Moreover the genre that we choose are slightly similar (i.e. for the thriller, action and horror  genres the difference in the plots is not as marked as it should be in order to train a well-prepared model for classification) in terms of plots. \n",
    "\n",
    "Summing up we are happy with the results that we have obtained and we think this has been a valuable exercise for our future experiences."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TEAMININGproject.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
